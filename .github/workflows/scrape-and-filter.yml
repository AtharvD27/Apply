name: Scrape & Filter Dice Jobs

on:
  schedule:
    - cron: '0 11 * * *'  # 7 AM EST
    - cron: '0 16 * * *'  # 12 PM EST
    - cron: '0 21 * * *'  # 5 PM EST
  workflow_dispatch:

jobs:
  scrape_and_filter:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repo
      uses: actions/checkout@v3
      with:
        persist-credentials: false
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install latest Chrome and matching Chromedriver
      run: |
        set -e  # exit immediately on error
        sudo apt-get update
        sudo apt-get install -y wget unzip curl gnupg lsb-release
    
        # Add Chrome repo
        wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo gpg --dearmor -o /usr/share/keyrings/google-linux.gpg
        echo "deb [arch=amd64 signed-by=/usr/share/keyrings/google-linux.gpg] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
    
        # Get installed Chrome version
        CHROME_VERSION=$(google-chrome --version | grep -oP '\d+\.\d+\.\d+')
        echo "Detected Chrome version: $CHROME_VERSION"
    
        # Get corresponding Chromedriver version
        DRIVER_VERSION=$(curl -s "https://chromedriver.storage.googleapis.com/LATEST_RELEASE_$CHROME_VERSION")
        echo "Downloading Chromedriver version: $DRIVER_VERSION"
    
        # Install Chromedriver
        wget -q "https://chromedriver.storage.googleapis.com/${DRIVER_VERSION}/chromedriver_linux64.zip"
        unzip -o chromedriver_linux64.zip
        sudo mv chromedriver /usr/local/bin/
        sudo chmod +x /usr/local/bin/chromedriver
    
        # Final version check
        which chromedriver || echo "Chromedriver not found"
        chromedriver --version || echo "Failed to verify chromedriver"
        google-chrome --version || echo "Failed to verify Chrome"


    - name: Run main.py to scrape and filter
      env:
        SCRAPER_EMAIL: ${{ secrets.SCRAPER_EMAIL }}
        SCRAPER_PASSWORD: ${{ secrets.SCRAPER_PASSWORD }}
        BASE_URL: ${{ secrets.BASE_URL }}
      run: python main.py

    - name: Commit and push updated job files
      run: |
        git config --global user.name "github-actions"
        git config --global user.email "github-actions@github.com"
        git add output/jobs.csv output/final_ml_jobs.csv

        if git diff --cached --quiet; then
          echo "No changes to commit."
        else
          git commit -m "ðŸ”„ Auto-update jobs.csv and final_ml_jobs.csv"
          git push https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git HEAD:${{ github.ref }}
